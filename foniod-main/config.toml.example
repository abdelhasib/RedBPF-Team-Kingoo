##########################
##### Logging
##########################
# Permitted log levels are `OFF`, `ERROR`, `WARN`, `INFO`, `DEBUG`, `TRACE`.
#
# If `EnvLogger` is used for `type`, the `RUST_LOG` environment
# variable will control the log level.
# For a full reference on EnvLogger capabilities, look at
# http://docs.rs/env_logger.
#
# If no `log` section is specified, the default backend is `EnvLogger`
[log]
type = "Syslog"
log_level = "INFO"

##########################
##### Probes and Grains
##########################
# For more information about the config file format, visit
# https://github.com/redsift/ingraind/wiki/Configuration
#
# For the list of tags generated by a grain, consult the grain documentation at
# https://github.com/redsift/ingraind/wiki/Configuration

# A list of all directories to monitor.
#
# If the directories are mounts, only the path relative to the _filesystem_ root
# are resolved. Eg. if /var is a different partition, events in /var/lib/docker
# would be returned as /lib/docker
#
[[probe]]
pipelines = ["console"]
[probe.config]
type = "Files"
monitor_dirs = ["/"]

# The Network grain will track outbound UDP and TCP connections, as well as
# send/receive metrics about established connections
# 
# Supports both IPv6 and IPv4, and will log all inbound UDP traffic.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "Network"

# The DNS grain monitors _inbpound_ DNS traffic only.
#
# On a local network, mDNS should also be picked up, as well as all incoming
# answers to outbound DNS queries
#
# A mandatory parameter is `interface`, which needs to specify the interface to
# monitor.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "DNS"
interface = "eth0"

# The TLS grain reports TLS ClientHello and ServerHello packets.

# A mandatory parameter is `interface`, which needs to specify the interface to
# monitor.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "TLS"
interface = "eth0"

# The Syscall grain allows tracking the frequency of specific system calls, system-wide.

# A mandatory parameter is `monitor_syscalls`, listing the system calls from which statistics will be collected.
# Using this on high-frequency syscalls (e.g. `read` or `write`) will impact CPU usage.
#
# Currently only x86_64 is supported.
# A full list of Linux syscalls can be found at https://filippo.io/linux-syscall-table/
[[probe]]
pipelines = ["console"]
[probe.config]
type = "Syscall"
monitor_syscalls = ["exit", "execve"]

# The StatsD grain allows receiving metrics from statsd clients.
#
# The bind address and flushing interval can be configured via the `bind_address` and `flush_interval` keys.
#
[[probe]]
pipelines = ["console"]
[probe.config]
type = "StatsD"
bind_address = "127.0.0.1:8125"
flush_interval = "10000"


# The osquery grain allows importing metrics from osquery.
#
# Osquery must be installed and osqueryi must be in the PATH for the grain to work.
#
[[probe]]
pipelines = ["console"]
[probe.config]
type = "Osquery"
interval_ms = 10000
[[probe.config.queries]]
query = "SELECT user_time, system_time, name from processes ORDER BY user_time DESC LIMIT 5"
name = "process_user_time"
measurement = "user_time"
measurement_type = "count"
run_at_start = true


##########################
##### Pipeline defintions
##########################
# The console backend dumps every incoming metric to stdout.
# 
# The output is Rust `Debug` objects, which cannot be directly parsed
# by any parser, but is reasonably human readable
[pipeline.console.config]
backend = "Console"

# The HTTP backend sends JSON data to the specified endpoint.
#
# All configuration is done through the config file, including authorization
# tokens, or other credential-style information.
#
# If the server returns with an error, there's no back-off mechanism, or any
# type of awareness of this.
[pipeline.http.config]
backend = "HTTP"
uri = "http://example.redsift.com/insert"
encoding = "JSON"
[pipeline.http.config.headers]
authorization = "token"
"custom-header" = "some value"


# The StatsD backend sends incoming metrics to a StatsD server using UDP.
#
# If the server supports Datadog extensions, then `use_tags` can be set to
# `true` to gather extended metadata.
#
# When running the program, `STATSD_HOST` and `STATSD_PORT` environment
# variables need to be set!
[pipeline.statsd.config]
backend = "StatsD"
use_tags = false

# The S3 backend sends incoming metrics to an S3 bucket.
# The files will contain a JSON array, and named like so:
#     hostname_<nanoseconds since UNIX epoch>
#
# It is recommended to use a `Buffer` step in S3 pipelines, to control how often
# a bucket is written.
#
# All configuration is runtime. The following environment variables MUST be set:
#  * AWS_ACCESS_KEY_ID=
#  * AWS_SECRET_ACCESS_KEY=
#  * AWS_S3_BUCKET=
#  * AWS_DEFAULT_REGION=
[pipeline.s3.config]
backend = "S3"

##########################
##### Aggregations/steps
##########################
# Steps will be walked through in order of definition. They may modify the
# content of the data coming from the grains, or omitted altogether.

# The AddSystemDetails filter will add `host` and `kernel` fields to
# the tags list, which carry the FQDN and the `release` field of
# `uname (2)`, respectively.
[[pipeline.s3.steps]]
type = "AddSystemDetails"

# The Exec aggregator will execute a command with the specified
# arguments for every event hitting the pipeline.
#
# Using the `{ tag_name }` syntax, the value of a tag can be substituted 
#
# If the `only_if` block is supplied, the ALL of the listed conditions
# need to match to execute the command (AND relationship)
[[pipeline.s3.steps]]
type = "Exec"
command = ["kill", "-9", "{ process_id }"]
only_if = [
  { key = "some_key", regex = "prefix.*" }
]

# The Whitelist filter will only preserve tags with the listed keys for a metric
# record.
[[pipeline.s3.steps]]
type = "Whitelist"
allow = ["k1", "k2"]

# The Regex filter will replace _values_ of tags with `replace_with`, where the
# _values_ match a particular `regex`. Matches are only made for the static
# `key`s described in the pattern entry.
#
# `replace_with` is a constant, and will disregard any regex pattern references.
[[pipeline.s3.steps]]
type = "Regex"
patterns = [
  { key = "some_key", regex = ".*", replace_with = "some_value"},
  { key = "some_key2", regex = ".*", replace_with = "some_value2"},
]

# The Container filter will parse the Docker container ID from the
# `/proc/<pid>/cgroup` file, and add a `docker_id` tag to the measurement.
#
# Supports an optional `system` key. When set to `Kubernetes`, it will add the
# `kubernetes_namespace` and `kubernetes_pod_name` tags. When set to `Docker`
# it will add the `docker_name` tag.
[[pipeline.s3.steps]]
type = "Container"
system = "Kubernetes"

# The Buffer aggregation will gather data for `interval_` seconds, and releases
# it to the next step only after.
# 
# All data points from the same metric with the same set of tags will be
# collapsed into one record, with the `timestamp` indicating the first record's
# `timestamp`.
#
# It makes sense to use the buffer step as last in a pipeline, and strip/filter
# any tags before they start filling up memory.
[[pipeline.statsd.steps]]
type = "Buffer"
interval_s = 30